# -*- coding: utf-8 -*-
"""kddcup.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QHqturwqrMYNdFDNTVJRjR3VRjFz1XtQ
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

features = [
    'duration',
    'protocol_type',
    'service',
    'flag',
    'src_bytes',
    'dst_bytes',
    'land',
    'wrong_fragment',
    'urgent',
    'hot',
    'num_failed_logins',
    'logged_in',
    'num_compromised',
    'root_shell',
    'su_attempted',
    'num_root',
    'num_file_creations',
    'num_shells',
    'num_access_files',
    'num_outbound_cmds',
    'is_host_login',
    'is_guest_login',
    'count',
    'srv_count',
    'serror_rate',
    'srv_serror_rate',
    'rerror_rate',
    'srv_rerror_rate',
    'same_srv_rate',
    'diff_srv_rate',
    'srv_diff_host_rate',
    'dst_host_count',
    'dst_host_srv_count',
    'dst_host_same_srv_rate',
    'dst_host_diff_srv_rate',
    'dst_host_same_src_port_rate',
    'dst_host_srv_diff_host_rate',
    'dst_host_serror_rate',
    'dst_host_srv_serror_rate',
    'dst_host_rerror_rate',
    'dst_host_srv_rerror_rate',
    'label'
]

data = pd.read_csv('/content/drive/My Drive/kddcup99_csv.csv', names=features, header=None)

print('The number of data points are:', data.shape[0])
print('=' * 40)

print('The number of features are:', data.shape[1])
print('=' * 40)

print('Some of the features are:', features[:10])

print('Null values in dataset are',len(data[data.isnull().any(1)]))

data.drop_duplicates(subset=features, keep='first', inplace = True)
data.shape

import matplotlib.pyplot as plt
plt.figure(figsize=(20,15))
class_distribution = data['label'].value_counts()
class_distribution.plot(kind='bar')
plt.xlabel('Class')
plt.ylabel('Data points per Class')
plt.title('Distribution of label in train data')
plt.grid()
plt.show()

import numpy as np

# Assuming your class labels are stored in a column named "label" in your DataFrame 'data'
class_labels = data['label'].unique()

sorted_yi = np.argsort(-class_distribution.values)

for i in sorted_yi:
    print('Number of data points in class', class_labels[i], ':', class_distribution.values[i], '(', np.round((class_distribution.values[i] / data.shape[0] * 100), 3), '%)')

import pandas as pd

print(data.columns)

import pandas as pd
from sklearn.model_selection import train_test_split


x = data.drop(columns=['label'])
y = data['label']

X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2, random_state=42)

print('Train data')
print(X_train.shape)
print(Y_train.shape)
print('='*20)
print('Test data')
print(X_test.shape)
print(Y_test.shape)
print('='*20)

protocol = list(X_train['protocol_type'].values)
protocol = list(set(protocol))
print('Protocol types are:', protocol)

from sklearn.feature_extraction.text import CountVectorizer
one_hot = CountVectorizer(vocabulary=protocol, binary=True)
train_protocol = one_hot.fit_transform(X_train['protocol_type'].values)
test_protocol = one_hot.transform(X_test['protocol_type'].values)
print(train_protocol[1].toarray())
print(train_protocol.shape)

from sklearn.feature_extraction.text import CountVectorizer

service_one_hot = CountVectorizer(binary=True)

service_one_hot.fit(X_train['service'].values)

train_service = service_one_hot.transform(X_train['service'].values)
test_service = service_one_hot.transform(X_test['service'].values)

flag_one_hot = CountVectorizer(binary=True)


flag_one_hot.fit(X_train['flag'].values)


train_flag = flag_one_hot.transform(X_train['flag'].values)
test_flag = flag_one_hot.transform(X_test['flag'].values)


print("One-hot encoded 'service' for the second sample in the training set:")
print(train_service[1].toarray())

print("One-hot encoded 'flag' for the second sample in the training set:")
print(train_flag[1].toarray())

print("Shapes of 'service' and 'flag' one-hot encoded arrays:")
print(train_service.shape, train_flag.shape)

X_train['src_bytes'] = pd.to_numeric(X_train['src_bytes'], errors='coerce')
X_test['src_bytes'] = pd.to_numeric(X_test['src_bytes'], errors='coerce')

X_train['duration'] = pd.to_numeric(X_train['duration'], errors='coerce')
X_test['duration'] = pd.to_numeric(X_test['duration'], errors='coerce')

X_train['dst_bytes'] = pd.to_numeric(X_train['dst_bytes'], errors='coerce')
X_test['dst_bytes'] = pd.to_numeric(X_test['dst_bytes'], errors='coerce')

import pywt
import pandas as pd

# Assuming your dataset is named 'data'
# Select only the numerical columns
numerical_columns = data.select_dtypes(include=['int', 'float'])

# Define the wavelet function and level of decomposition
wavelet = 'db1'  # You can choose a different wavelet as needed
level = 3  # Choose the level of decomposition

# Apply wavelet transformation to each column
transformed_data = []
for column in numerical_columns:
    coeffs = pywt.wavedec(data[column], wavelet, level=level)
    for coeff in coeffs:
        transformed_data.append(coeff)

# Create a new DataFrame with the transformed data
transformed_df = pd.DataFrame(transformed_data).transpose()

# Concatenate the transformed DataFrame with the non-numerical columns
# Assuming non-numerical columns are stored in data_non_numerical
final_df = pd.concat([transformed_df, data.select_dtypes(exclude=['int', 'float'])], axis=1)

from sklearn.preprocessing import StandardScaler

def feature_scaling(X_train, X_test, feature_name):
    scaler = StandardScaler()
    scaler1 = scaler.fit_transform(X_train[feature_name].values.reshape(-1, 1))
    scaler2 = scaler.transform(X_test[feature_name].values.reshape(-1, 1))
    return scaler1, scaler2

# 1. Duration
duration1, duration2 = feature_scaling(X_train, X_test, 'duration')
print(duration1[1])

# 2. src_bytes
src_bytes1, src_bytes2 = feature_scaling(X_train, X_test, 'src_bytes')
print(src_bytes1[1])

# 3. dst_bytes
dst_bytes1, dst_bytes2 = feature_scaling(X_train, X_test, 'dst_bytes')
print(dst_bytes1[1])

import numpy as np
from scipy.sparse import hstack


X_train_combined = hstack((duration1, src_bytes1, dst_bytes1, train_service, train_flag))


print("Shape of the combined training dataset:", X_train_combined.shape)

X_test_combined = hstack((duration2, src_bytes2, dst_bytes2, test_service, test_flag))

print("Shape of the combined test dataset:", X_test_combined.shape)

import pandas as pd


categorical_columns = ['protocol_type', 'service', 'flag']

X_train_encoded = pd.get_dummies(X_train, columns=categorical_columns, drop_first=True)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder
from sklearn.utils import class_weight
import pandas as pd
from sklearn.metrics import confusion_matrix
X_train_encoded = X_train_encoded.astype(str)
encoder = LabelEncoder()
X_train_encoded = X_train_encoded.apply(encoder.fit_transform)
class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(Y_train), y=Y_train)
KNN_Classifier = KNeighborsClassifier()
param_grid = {'n_neighbors': [3, 5, 7, 9, 11]}
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
grid_search = GridSearchCV(KNN_Classifier, param_grid, cv=cv, scoring='accuracy')
grid_search.fit(X_train_encoded, Y_train)
best_n_neighbors = grid_search.best_params_['n_neighbors']
best_KNN_Classifier = KNeighborsClassifier(n_neighbors=best_n_neighbors)
best_KNN_Classifier.fit(X_train_encoded, Y_train)
y_pred_best_knn = best_KNN_Classifier.predict(X_train_encoded)
classification_rep_best_knn = classification_report(Y_train, y_pred_best_knn, zero_division='warn')
print(f'Best n_neighbors found: {best_n_neighbors}')
print('Classification Report-KNN:')
print(classification_rep_best_knn)

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
# Compute confusion matrix
conf_mat_best_knn = confusion_matrix(Y_train, y_pred_best_knn)

# Plot confusion matrix as a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(conf_mat_best_knn, annot=True, fmt='d', cmap='Blues', xticklabels=best_KNN_Classifier.classes_, yticklabels=best_KNN_Classifier.classes_)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix ')
plt.show()

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.metrics import classification_report
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import RandomOverSampler
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix
import seaborn as sns
# Define the Decision Tree Classifier
DTC_Classifier = DecisionTreeClassifier()

# Define the parameter grid for tuning
param_grid = {
    'classifier__max_depth': [None, 10, 20],  # Specify max_depth for the DecisionTreeClassifier
    'classifier__min_samples_split': [2, 5, 10],
    'classifier__min_samples_leaf': [1, 2, 4]
}

# Define the pipeline with oversampling and classifier
pipeline = Pipeline([
    ('oversample', RandomOverSampler()),  # Oversampling to handle class imbalance
    ('scaler', StandardScaler()),  # Standardize features
    ('classifier', DTC_Classifier)
])

# Initialize StratifiedKFold with 5 splits
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Initialize GridSearchCV with the pipeline and parameter grid
grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring='accuracy')

# Perform grid search to find the best parameters
grid_search.fit(X_train_encoded, Y_train)

# Get the best parameters
best_params = grid_search.best_params_

# Get the best Decision Tree Classifier with the best parameters
best_DTC_Classifier = grid_search.best_estimator_

# Fit the best classifier to the entire training dataset
best_DTC_Classifier.fit(X_train_encoded, Y_train)

# Make predictions using the best classifier
y_pred_best_dt = best_DTC_Classifier.predict(X_train_encoded)

# Generate a classification report with the best classifier
classification_rep_best_dt = classification_report(Y_train, y_pred_best_dt, zero_division='warn')
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

# Plot the decision tree
plt.figure(figsize=(15, 10))
plot_tree(best_DTC_Classifier.named_steps['classifier'], feature_names=X_train_encoded.columns, class_names=best_DTC_Classifier.named_steps['classifier'].classes_, filled=True)
plt.show()

# Print the results
print(f'Best parameters found: {best_params}')
print('Classification Report (Best Decision Tree):')
print(classification_rep_best_dt)

# Compute confusion matrix
conf_mat_best_dt = confusion_matrix(Y_train, y_pred_best_dt)

# Plot confusion matrix as a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(conf_mat_best_dt, annot=True, fmt='d', cmap='Blues', xticklabels=best_DTC_Classifier.classes_, yticklabels=best_DTC_Classifier.classes_)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
# Create a Random Forest Classifier
RF_Classifier = RandomForestClassifier()

# Define the parameter grid for tuning
param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=RF_Classifier, param_grid=param_grid, cv=5)

# Perform grid search
grid_search.fit(X_train_encoded, Y_train)

# Get the best parameters and best model
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_

# Make predictions using the best model
y_pred_rf_best = best_model.predict(X_train_encoded)

# Calculate mean validation accuracy
mean_validation_accuracy_rf_best = (y_pred_rf_best == Y_train).mean()

# Generate a classification report for the best model
classification_rep_rf_best = classification_report(Y_train, y_pred_rf_best, zero_division='warn')

# Print the results
print("Best parameters:", best_params)
print(f'Mean Validation Accuracy (Random Forest - Best Model): {mean_validation_accuracy_rf_best:.2f}')
print('Classification Report (Random Forest - Best Model):')
print(classification_rep_rf_best)
conf_matrix_rf_best = confusion_matrix(Y_train, y_pred_rf_best)
plt.figure(figsize=(12, 8))
sns.heatmap(conf_matrix_rf_best, annot=True, fmt="d", cmap="Blues", xticklabels=best_model.classes_, yticklabels=best_model.classes_)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix ')
plt.show()

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.utils import class_weight

# Compute class weights to handle class imbalance
class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(Y_train), y=Y_train)

# Create a Gaussian Naive Bayes Classifier
NB_Classifier = GaussianNB()

# Fit the classifier to the training data with adjusted class priors
NB_Classifier.class_prior_ = class_weights

# Fit the classifier to the training data
NB_Classifier.fit(X_train_encoded, Y_train)

# Make predictions on the training data
y_pred_nb = NB_Classifier.predict(X_train_encoded)

# Generate a classification report
classification_rep_nb = classification_report(Y_train, y_pred_nb, zero_division='warn')

# Print the classification report
print('Classification Report (Naive Bayes):')
print(classification_rep_nb)

# Compute and plot the confusion matrix
conf_matrix_nb = confusion_matrix(Y_train, y_pred_nb)
plt.figure(figsize=(12, 8))
sns.heatmap(conf_matrix_nb, annot=True, fmt="d", cmap="Blues", xticklabels=NB_Classifier.classes_, yticklabels=NB_Classifier.classes_)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix (Naive Bayes)')
plt.show()

import pandas as pd

# Load the data from the CSV file
data = pd.read_csv('/content/drive/My Drive/kddcup99_csv.csv')

# Print the labels
print("Labels:")
print(data['label'].unique())